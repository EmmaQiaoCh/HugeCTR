# Please do not edit this file
import os
import json
import glob
from collections import OrderedDict

#import numpy as np

DRLM_EVENTS = {
    # interested event name
    'BottomMLP.fc1': {
        'same_name_events_occured_order_in_code_forward': 0,
        'same_name_events_occured_order_in_code_backward': 6,
    },
    'BottomMLP.fc2': {
        'same_name_events_occured_order_in_code_forward': 1,
        'same_name_events_occured_order_in_code_backward': 5,
    },
    'BottomMLP.fc3': {
        'same_name_events_occured_order_in_code_forward': 2,
        'same_name_events_occured_order_in_code_backward': 4,
    },
    'sparse_embedding1': {
        'same_name_events_occured_order_in_code_forward': 0,
        'same_name_events_occured_order_in_code_backward': 0,
    },
    'interaction1': {
        'same_name_events_occured_order_in_code_forward': 0,
        'same_name_events_occured_order_in_code_backward': 0,
    },
    'TopMLP.fc4': {
        'same_name_events_occured_order_in_code_forward': 3,
        'same_name_events_occured_order_in_code_backward': 3,
    },
    'TopMLP.fc5': {
        'same_name_events_occured_order_in_code_forward': 4,
        'same_name_events_occured_order_in_code_backward': 2,
    },
    'TopMLP.fc6': {
        'same_name_events_occured_order_in_code_forward': 5,
        'same_name_events_occured_order_in_code_backward': 1,
    },
    'TopMLP.fc7': {
        'same_name_events_occured_order_in_code_forward': 6,
        'same_name_events_occured_order_in_code_backward': 0,
    },
    'TopMLP.fc8': {
        'same_name_events_occured_order_in_code_forward': 0,
        'same_name_events_occured_order_in_code_backward': 0,
    },
    'Loss' : {
        'same_name_events_occured_order_in_code_forward': 0,
    },
    'AllReduce_wgrads' : {
        'same_name_events_occured_order_in_code_forward': 0,
    },
    'Update_Params': {
        'same_name_events_occured_order_in_code_forward': 0,
    }
}


def generate_schedule(schedule, profiling_dir, repeat_for_each_event=50, warmup_iterations=10):
    with open(os.path.join(profiling_dir, 'prof.schedule'), 'wb') as f:
        f.write(str(warmup_iterations).encode('ascii', 'ignore'))
        iteration = warmup_iterations + 1
        for layer in schedule:
            for f_or_b_event in schedule[layer].keys():
                for interested_event in schedule[layer][f_or_b_event]:
                    if f_or_b_event == 'forward_events':
                        same_name_events_occured_order_in_code = DRLM_EVENTS[layer]['same_name_events_occured_order_in_code_forward']
                    elif f_or_b_event == 'backward_events':
                        same_name_events_occured_order_in_code = DRLM_EVENTS[layer]['same_name_events_occured_order_in_code_backward']
                    else:
                        raise Exception("{} not a forward_event nor a backward_event".format(interested_event))
                    for _ in range(repeat_for_each_event):
                        line = "\n{} {} {} {}".format(
                            interested_event,
                            iteration,
                            layer,
                            same_name_events_occured_order_in_code
                        ).encode('ascii', 'ignore')
                        f.write(line)
                        iteration += 1

def parse_result(profiling_dir):
    prof_jsons = glob.glob(os.path.join(profiling_dir, '*.prof.json'))
    ret = []
    for prof_file in prof_jsons:
        with open(prof_file, 'r') as f:
            jstring = f.read()
            prof = json.loads(jstring)
            timeline = prof['events']
            timeline.sort(key=lambda e: e["start_index"])
            result = {
                'host_name': prof['host_name'],
                'avg_iter_time_ms': sum(prof['iter_time_ms']) / len(prof['iter_time_ms']),
                'timeline': split_by_device_stream_layer_label(timeline)
            }
        ret.append(result)
    return ret

def split_by_layer_device_stream(events):
    # events is a array
    global_streams = []
    result = OrderedDict()
    for event in timeline:
        layer_name = event["layer_name"]
        if layer_name not in result.keys():
            result[layer_name] = OrderedDict()
        device_id = "device_" + str(event["device_id"])
        if device_id not in result[layer_name].keys():
            result[layer_name][device_id] = OrderedDict()
        if event["stream"] in global_streams:
            stream_id = "stream_" + str(global_streams.index(event["stream"]))
        else:
            stream_id = "stream_" + str(len(global_streams))
            global_streams.append(event["stream"])
        if stream_id not in result[layer_name][device_id].keys():
            result[layer_name][device_id][stream_id] = []
        new_event = OrderedDict()
        new_event["name"] = event["name"]
        new_event["start_index"] = event["start_index"]
        new_event["end_index"] = event["end_index"]

        #measured_times_ms = reject_outliers(event["measured_times_ms"])
        measured_times_ms = event["measured_times_ms"]
        new_event["avg_measured_time_ms"] = sum(measured_times_ms) / len(measured_times_ms)
        #iter_start_to_event_start_times_ms = reject_outliers(event["iter_start_to_event_start_times_ms"])
        iter_start_to_event_start_times_ms = event["iter_start_to_event_start_times_ms"]
        new_event["avg_iter_start_to_event_start_time_ms"] = sum(iter_start_to_event_start_times_ms) / len(iter_start_to_event_start_times_ms)
        result[layer_name][device_id][stream_id].append(new_event)
    return result

def split_by_device_stream_layer_label(events):
    global_streams = []
    result = OrderedDict()
    for event in events:
        device_id = "device_" + str(event["device_id"])
        if device_id not in result.keys():
            result[device_id] = OrderedDict()
        if event["stream"] in global_streams:
            stream_id = "stream_" + str(global_streams.index(event["stream"]))
        else:
            stream_id = "stream_" + str(len(global_streams))
            global_streams.append(event["stream"])
        if stream_id not in result[device_id].keys():
            result[device_id][stream_id] = []
        new_event = OrderedDict()
        new_event["label"] = event["layer_name"] + '.' + event['name']
        #new_event["start_index"] = event["start_index"]
        #new_event["end_index"] = event["end_index"]

        #measured_times_ms = reject_outliers(event["measured_times_ms"])
        measured_times_ms = event["measured_times_ms"]
        new_event["avg_measured_time_ms"] = sum(measured_times_ms) / len(measured_times_ms)
        #iter_start_to_event_start_times_ms = reject_outliers(event["iter_start_to_event_start_times_ms"])
        iter_start_to_event_start_times_ms = event["iter_start_to_event_start_times_ms"]
        new_event["avg_iter_start_to_event_start_time_ms"] = sum(iter_start_to_event_start_times_ms) / len(iter_start_to_event_start_times_ms)
        result[device_id][stream_id].append(new_event)
    for _, device_events in result.items():
        for _, stream_events in device_events.items():
            stream_events.sort(key=lambda e: e["avg_iter_start_to_event_start_time_ms"])
    return result


#def reject_outliers(data, m=2.):
#    data = np.array(data)
#    return data[abs(data - np.mean(data)) < m * np.std(data)].tolist()